{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNsnusE0MeVV",
        "outputId": "ab3997d4-0ea6-4f43-9c63-280b82822446"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique names to resolve: 68,958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GBIF matching: 100%|██████████| 68958/68958 [32:39<00:00, 35.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote gbif_taxonomy.csv with 68,958 rows.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import queue\n",
        "import signal\n",
        "import string\n",
        "import hashlib\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from tqdm import tqdm\n",
        "\n",
        "INPUT_CSV = \"filtered_combined_species.csv\"\n",
        "OUTPUT_CSV = \"gbif_taxonomy.csv\"\n",
        "CACHE_JSONL = \"gbif_match_cache.jsonl\"   # name→raw /species/match response\n",
        "DETAILS_CACHE_JSONL = \"gbif_details_cache.jsonl\"  # usageKey→/species/{key} detail\n",
        "\n",
        "# ---- Settings you can tune ----\n",
        "MAX_WORKERS = 8          # polite parallelism; lower if you see 429s\n",
        "REQ_TIMEOUT = 12         # seconds per HTTP request\n",
        "USER_AGENT = \"victoria.t.tiki@gmail.com (Pelagica 1.0)\"\n",
        "SLEEP_BETWEEN_BATCHES = 0.0  # seconds; increase if you still see 429s\n",
        "# --------------------------------\n",
        "\n",
        "MATCH_URL = \"https://api.gbif.org/v1/species/match\"\n",
        "DETAIL_URL = \"https://api.gbif.org/v1/species/{}\"\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": USER_AGENT})\n",
        "\n",
        "# ---- simple append-only JSONL caches (restart-safe) ----\n",
        "def _load_jsonl(path, key_field):\n",
        "    data = {}\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    obj = json.loads(line)\n",
        "                    data[obj[key_field]] = obj[\"value\"]\n",
        "                except Exception:\n",
        "                    continue\n",
        "    return data\n",
        "\n",
        "def _append_jsonl(path, key, value):\n",
        "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps({key: key, \"value\": value}) + \"\\n\")\n",
        "\n",
        "match_cache = _load_jsonl(CACHE_JSONL, key_field=\"name\")\n",
        "details_cache = _load_jsonl(DETAILS_CACHE_JSONL, key_field=\"key\")\n",
        "\n",
        "class HttpRetryableError(Exception):\n",
        "    pass\n",
        "\n",
        "@retry(\n",
        "    reraise=True,\n",
        "    stop=stop_after_attempt(6),\n",
        "    wait=wait_exponential(multiplier=0.8, min=1, max=60),\n",
        "    retry=retry_if_exception_type(HttpRetryableError),\n",
        ")\n",
        "def _get(url, params=None):\n",
        "    resp = session.get(url, params=params, timeout=REQ_TIMEOUT)\n",
        "    # 2xx ok\n",
        "    if 200 <= resp.status_code < 300:\n",
        "        return resp.json()\n",
        "    # backoff on 429/5xx\n",
        "    if resp.status_code in (429, 500, 502, 503, 504):\n",
        "        raise HttpRetryableError(f\"HTTP {resp.status_code}\")\n",
        "    # other codes: raise immediately\n",
        "    resp.raise_for_status()\n",
        "\n",
        "def match_name(scientific_name):\n",
        "    # cache\n",
        "    if scientific_name in match_cache:\n",
        "        return match_cache[scientific_name]\n",
        "\n",
        "    params = {\n",
        "        \"name\": scientific_name,\n",
        "        # strict=false enables helpful fuzzy matching for messy lists\n",
        "        \"strict\": \"false\",\n",
        "        # verbose adds classification fields when available\n",
        "        \"verbose\": \"true\",\n",
        "    }\n",
        "    data = _get(MATCH_URL, params=params)\n",
        "    match_cache[scientific_name] = data\n",
        "    _append_jsonl(CACHE_JSONL, \"name\", {\"name\": scientific_name, **{\"value\": data}}[\"value\"])\n",
        "    return data\n",
        "\n",
        "def get_detail(usage_key):\n",
        "    k = str(usage_key)\n",
        "    if k in details_cache:\n",
        "        return details_cache[k]\n",
        "    data = _get(DETAIL_URL.format(usage_key))\n",
        "    details_cache[k] = data\n",
        "    _append_jsonl(DETAILS_CACHE_JSONL, \"key\", {\"key\": k, **{\"value\": data}}[\"value\"])\n",
        "    return data\n",
        "\n",
        "def normalize_row(scientific_name, match_json):\n",
        "    \"\"\"Return a dict of normalized taxonomy fields for output CSV.\"\"\"\n",
        "    base = {\n",
        "        \"scientificName\": scientific_name,\n",
        "        \"gbifID\": None,\n",
        "        \"status\": None,\n",
        "        \"matchType\": None,\n",
        "        \"confidence\": None,\n",
        "        \"kingdom\": None,\n",
        "        \"phylum\": None,\n",
        "        \"class\": None,\n",
        "        \"order\": None,\n",
        "        \"family\": None,\n",
        "        \"genus\": None,\n",
        "        \"species\": None,\n",
        "        \"acceptedScientificName\": None,\n",
        "    }\n",
        "\n",
        "    if not match_json or \"usageKey\" not in match_json:\n",
        "        return base  # unmatched\n",
        "\n",
        "    # Fields from /species/match\n",
        "    base[\"gbifID\"] = match_json.get(\"usageKey\")\n",
        "    base[\"status\"] = match_json.get(\"status\")\n",
        "    base[\"matchType\"] = match_json.get(\"matchType\")\n",
        "    base[\"confidence\"] = match_json.get(\"confidence\")\n",
        "\n",
        "    # If a synonym, follow acceptedUsageKey to get the “current/accepted” classification\n",
        "    accepted_key = match_json.get(\"acceptedUsageKey\")\n",
        "    if accepted_key:\n",
        "        detail = get_detail(accepted_key)\n",
        "        base[\"acceptedScientificName\"] = detail.get(\"scientificName\")\n",
        "        base[\"gbifID\"] = detail.get(\"key\", base[\"gbifID\"])\n",
        "        # fill classification from accepted detail\n",
        "        for rank in (\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"):\n",
        "            base[rank] = detail.get(rank)\n",
        "        return base\n",
        "\n",
        "    # Otherwise, fill classification from match payload (often present with verbose=true).\n",
        "    for rank in (\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"):\n",
        "        base[rank] = match_json.get(rank)\n",
        "\n",
        "    # If any are missing, fetch /species/{usageKey} to complete\n",
        "    if not all(base[r] for r in (\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\")):\n",
        "        detail = get_detail(base[\"gbifID\"])\n",
        "        for rank in (\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"):\n",
        "            base[rank] = base[rank] or detail.get(rank)\n",
        "        base[\"acceptedScientificName\"] = detail.get(\"scientificName\")\n",
        "\n",
        "    return base\n",
        "\n",
        "def read_input_names(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # keep only Genus + Species; ignore the rest\n",
        "    if \"Genus\" not in df.columns or \"Species\" not in df.columns:\n",
        "        raise ValueError(\"Input CSV must contain 'Genus' and 'Species' columns.\")\n",
        "    tmp = df[[\"Genus\", \"Species\"]].copy()\n",
        "    tmp[\"Genus\"] = tmp[\"Genus\"].astype(str).str.strip()\n",
        "    tmp[\"Species\"] = tmp[\"Species\"].astype(str).str.strip()\n",
        "    tmp[\"scientificName\"] = tmp[\"Genus\"] + \" \" + tmp[\"Species\"]\n",
        "    # de-duplicate to minimize API calls\n",
        "    names = sorted(tmp[\"scientificName\"].dropna().unique().tolist())\n",
        "    return names\n",
        "\n",
        "def process_names(names):\n",
        "    results = []\n",
        "\n",
        "    def worker(name):\n",
        "        mj = match_name(name)\n",
        "        row = normalize_row(name, mj)\n",
        "        return row\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futures = {ex.submit(worker, nm): nm for nm in names}\n",
        "        for i, fut in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"GBIF matching\")):\n",
        "            try:\n",
        "                results.append(fut.result())\n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    \"scientificName\": futures[fut],\n",
        "                    \"gbifID\": None, \"status\": None, \"matchType\": None, \"confidence\": None,\n",
        "                    \"kingdom\": None, \"phylum\": None, \"class\": None, \"order\": None,\n",
        "                    \"family\": None, \"genus\": None, \"species\": None, \"acceptedScientificName\": None,\n",
        "                    \"error\": str(e),\n",
        "                })\n",
        "            if SLEEP_BETWEEN_BATCHES:\n",
        "                time.sleep(SLEEP_BETWEEN_BATCHES)\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    names = read_input_names(INPUT_CSV)\n",
        "    print(f\"Unique names to resolve: {len(names):,}\")\n",
        "\n",
        "    rows = process_names(names)\n",
        "\n",
        "    # save as tidy DataFrame\n",
        "    out_df = pd.DataFrame(rows, columns=[\n",
        "        \"scientificName\", \"gbifID\", \"status\", \"matchType\", \"confidence\",\n",
        "        \"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\",\n",
        "        \"acceptedScientificName\"\n",
        "    ])\n",
        "    out_df.to_csv(OUTPUT_CSV, index=False)\n",
        "    print(f\"Wrote {OUTPUT_CSV} with {len(out_df):,} rows.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dP69539FM4FI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}